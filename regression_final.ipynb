{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":5,"outputs":[{"output_type":"stream","text":"/kaggle/input/mkn-ml-2020-competition-1-part-1/comp1_test.csv\n/kaggle/input/mkn-ml-2020-competition-1-part-1/comp1_train.csv\n/kaggle/input/mkn-ml-2020-competition-1-part-1/comp1_sample.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class StupidLinearRegression:\n    def __init__(self, Features_amount):\n        self.features_amount = Features_amount + 1\n        self.weights = np.zeros((Features_amount + 1, 1))\n        self.alpha = 1\n    def set_weights(self, Weights):\n        self.features_amount = np.shape(Weights)[0]\n        self.weights = Weights\n    def set_alpha(self, Alpha):\n        self.alpha = Alpha\n    def predict(self, X):\n        X_predict = X.copy()\n        X_predict['const'] = 1\n        return np.dot(X_predict, self.weights)\n    def straight_train(self, X, Y):\n        X_train = X.copy()\n        X_train['const'] = 1\n        X_sq = np.dot(np.transpose(X_train), X_train)\n        Id = np.identity(np.shape(X_sq)[0])\n        X_sq = X_sq + Id * self.alpha\n        X_inv = np.linalg.inv(X_sq)\n        Ans = np.dot(X_inv, np.transpose(X_train))\n        self.weights = np.dot(Ans, Y)\n    def zeroing(self):\n        self.weights = np.zeros((self.features_amount, 1))\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Mean_Square(model, X, Y): # mse\n    Ans = model.predict(X)\n    #print(np.array(np.mean((Ans - Y)*(Ans - Y))))\n    return np.array(np.mean((Ans - Y)*(Ans - Y)))\n\n#random cross validation with parameter k, np.shale(X)[0] / k is the amount of parts\ndef cross_validation(model, X, Y, k):\n    #ind = np.random.choice(np.arange(len(X)), 1000)\n    Results = np.zeros(k)\n    X_copy = X.copy(deep=True)\n    Y_copy = Y.copy(deep=True)\n    #X_copy = X_copy.loc[ind]\n    #Y_copy = Y_copy.loc[ind] + np.random.normal(size = 1000)                     \n    Amount_of_strings = np.shape(X_copy)[0]\n    Step = Amount_of_strings // k\n\n    Strings_indices = np.arange(Amount_of_strings)\n    cross_val_ind = np.random.randint(k, size = len(X_copy))\n    for i in range(k):\n        X_Train = X_copy.loc[cross_val_ind != i]\n        X_Test = X_copy.loc[cross_val_ind == i]\n        \n        Y_Train = Y_copy.loc[cross_val_ind != i]\n        Y_Test = Y_copy.loc[cross_val_ind == i]\n        \n        model.straight_train(X_Train, Y_Train)\n        Results[i] = Mean_Square(model, X_Test, Y_Test)\n        model.zeroing\n    return np.mean(Results)\n\ndef forward_subset_selection(model, X, Y, F_amount): #choosing features one after\n    Results = pd.DataFrame()\n    for i in range(F_amount):\n        X_cut = X.copy(deep = True)\n        X_cut = X.iloc[:, 0:(i + 1)]\n        W = np.zeros(i + 2)\n        model.set_weights(W)\n        \n        tmp = cross_validation(model, X_cut, Y, 100)\n    return Results\n\ndef full_subset_selection(model, X, Y, k): #considering all feature iteration\n    Border = 2**12 - 1\n    Results = [0 for i in range(Border - 1)]\n\n    for i in range(Border - 1):\n        T = [False for j in range(12)]\n        for j in range(12):\n            Check = 2**j\n            if (i + 1 & Check == Check):\n                T[j] = True\n        X_cut = X.iloc[:, T]\n        \n        trues = 0\n        for j in range(12):\n            if T[j] == True:\n                trues += 1\n        W = np.zeros(trues + 1)\n        Results[i] = [cross_validation(model, X_cut, Y, k)]\n    Min = 100\n    Min_index = 0\n    for i in range(Border - 1):\n        if Results[i][0] < Min:\n            Min = Results[i][0]\n            Min_index = i\n    return (Min, Min_index + 1)\n    \ndef wridge_regularization(model, X, Y, Bottom_border, Top_border, Amount_of_steps):#choosing alpha\n    Results = pd.DataFrame()\n    for i in range(Amount_of_steps):\n        alpha = Bottom_border + (i + 1) * ((Top_border - Bottom_border) / Amount_of_steps)\n        if (alpha == 0):\n            model.set_alpha(1)\n        else:\n            model.set_alpha(alpha)\n        Results[str(alpha)] = [cross_validation(model, X, Y, 100)]\n    return Resultshg","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data = pd.read_csv('/kaggle/input/mkn-ml-2020-competition-1-part-1/comp1_train.csv')\nData_X = Data.iloc[:, 1:]\nData_Y = Data.iloc[:, 0:1]","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"import matplotlib.pyplot as plt #from here we can see that we need to add 12 features that has type of np.sin('feature_i')\nfor i in range(12):    \n    f = plt.figure()\n    X = Data_X.iloc[:, i:(i+1)]\n    Y = Data_Y\n\n    plt.title('feature ' + str(i + 1))\n    plt.scatter(X, Y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Best_SLR = StupidLinearRegression(24) #using this method we can drop unnecessary features and sin(features). The main idea is to\n#train model 1000 times train data sets with train quoting to test equal 69 to 31, and look on expected value and disspersion on\n#weights. If exected value == 0 or expected value ~ dispersion then this feature is unnecessary\nData_X_sin = Data_X.copy(deep = True)\nfor i in range(12):\n    Data_X_sin['sin_' + str(i + 1)] = np.sin(Data_X['feature_' + str(i + 1)])\nResult = []\nfor i in range(1000):\n    ind = np.random.choice(np.arange(len(Data_X)), size = 69, replace = False)\n    X_far = Data_X_sin.loc[ind].copy().drop(['sin_2', 'sin_3', 'feature_6', 'sin_4', 'sin_5', 'sin_7', 'sin_11', 'feature_2', 'feature_9', 'sin_9', \n                                            'feature_4', 'feature_8', 'feature_12', 'feature_11'], axis = 1)\n    Y_far = Data_Y.loc[ind].copy()\n    Best_SLR.zeroing\n    Best_SLR.straight_train(X_far, Y_far)\n    Result.append(Best_SLR.weights)\nResult = np.hstack(Result)\nMean = np.mean(Result, axis = 1)\nStd = np.std(Result, axis = 1)\nfor i, col in enumerate(X_far.columns):\n    print(f\"{col}: {Mean[i]:.4f} ± {Std[i]:.4}\")","execution_count":10,"outputs":[{"output_type":"stream","text":"feature_1: 1.6874 ± 0.0161\nfeature_3: 2.0903 ± 0.01172\nfeature_5: -1.3436 ± 0.01302\nfeature_7: 0.3177 ± 0.01449\nfeature_10: 0.7429 ± 0.01073\nsin_1: 4.2638 ± 0.1081\nsin_6: -0.1051 ± 0.03667\nsin_8: -0.2078 ± 0.08041\nsin_10: -0.1051 ± 0.03667\nsin_12: 5.2262 ± 0.1208\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Subm_SLR = StupidLinearRegression(24)\nData_X_sin = Data_X.copy(deep = True)\nfor i in range(12):\n    Data_X_sin['sin_' + str(i + 1)] = np.sin(Data_X['feature_' + str(i + 1)])\nData_X_sin = Data_X_sin.drop(['sin_2', 'sin_3', 'feature_6', 'sin_4', 'sin_5', 'sin_7', 'sin_11', 'feature_2', 'feature_9', 'sin_9', \n                                             'feature_12', 'feature_11'], axis = 1)\nSubm_SLR.straight_train(Data_X_sin, Data_Y)\n\nTest = pd.read_csv('/kaggle/input/mkn-ml-2020-competition-1-part-1/comp1_test.csv')\nTest_X = Test.iloc[:, 0:12]\nTest_X_sin = Test_X.copy(deep = True)\nfor i in range(12):\n    Test_X_sin['sin_' + str(i + 1)] = np.sin(Test_X['feature_' + str(i + 1)])\nTest_X_sin = Test_X_sin.drop(['sin_2', 'sin_3', 'feature_6', 'sin_4', 'sin_5', 'sin_7', 'sin_11', 'feature_2', 'feature_9', 'sin_9', \n                              'feature_12', 'feature_11'], axis = 1)\nsubmission = pd.DataFrame()\nsubmission['Id'] = Test.index\nsubmission['target'] = Subm_SLR.predict(Test_X_sin)\nsubmission = submission[['target', 'Id']]\nsubmission.to_csv('submission_final.csv', index=False)#this submision is the best one.\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SLR = StupidLinearRegression(12)\nSLR.straight_train(Data_X, Data_Y)\n\nsubmission = pd.DataFrame()\nsubmission['Id'] = Test.index\nsubmission['target'] = SLR.predict(Test_X)\nsubmission = submission[['target', 'Id']]\nsubmission.to_csv('submission_first.csv', index=False)#this submision is the first one and second best.\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}